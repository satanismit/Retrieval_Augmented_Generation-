# -*- coding: utf-8 -*-
"""Retrieval_Augmented_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PE4T_GFKjyvCclZbsvN86LGR0zDxtZp0
"""

! pip install langchain_community langchainhub chromadb langchain langchain-openai

"""**DATA** **SCRAP**"""

# ## Document Loader :

# from langchain_community.document_loaders import WebBaseLoader

# urls = [
#     "https://www.healthline.com/health/india",
#     "https://www.hopkinsmedicine.org/health",
#     "https://www.webmd.com/a-to-z-guides/health-topics",
#     "https://newsinhealth.nih.gov/",
#     "https://indianexpress.com/section/lifestyle/health/"
# ]

# loader = WebBaseLoader(web_path=urls)
# data = loader.load()

# print(data.__len__())

from langchain.schema import Document

texts = [
    """ Patient Name: Mr. Dummy

â€˜Age/Sex: 23 YRS/M

Referred By: _Dr. Self Date: 14/05/2021 TIN
Reg. no. 1024 UHID: 1028
Collected on: 14/05/2021 Reported on: 14/05/2021 03:03 PM
HAEMATOLOGY
COMPLETE BLOOD COUNT (CBC)
TEST VALUE UNIT REFERENCE
Hemoglobin 14 g/dl 13-17
Total Leukocyte Count H 12,000 cumm 4,000 - 11,000
Differential Leucocyte Count
Neutrophils 45 % 40-80
Lymphocyte H 45 % 20-40
Eosinophils 05 % 1-6
Monocytes 05 % 2-10
Basophils 00 % <2
Platelet Count 40 lakhs/cumm 15-45
Total RBC Count 51 million/cumm 45-55
Hematocrit Value, Het H 56 % 40-50
Mean Corpuscular Volume, MCV H 109.8 fL 83-101
Mean Cell Haemoglobin, MCH 275 Pg 27-32
Mean Cell Haemoglobin CON,MCHC L 25.0 % 315-345

"""
]

# Convert list of raw texts into list of Document objects
documents = [Document(page_content=text) for text in texts]

"""**TEXT** **SPLITTER**"""

## split Our Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

texts = text_splitter.split_documents(documents)

print(len(texts))

"""**EMBEDDING**"""

## Embedd every split text to vector
from langchain.vectorstores.chroma import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vectorstore = Chroma.from_documents(documents=texts, embedding=embedding_model)

# count embedding chuunks
print(vectorstore._collection.count())

# get all chunks embedding ids
print(vectorstore._collection.get())

# Get the document chunk and its embedding
print("Collection 1", vectorstore._collection.get(ids=['ab392192-b90d-4016-8df2-7bdfac3131cb'], include=['embeddings','documents']))

"""**RAG** **PIPELINE**"""

retriever = vectorstore.as_retriever()

## get prompt from langchain.hub

from langchain import hub

prompt = hub.pull("rlm/rag-prompt")

"""**Create** **LLM**"""

!pip install langchain-google-genai

from google.colab import userdata
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI

llm =ChatGoogleGenerativeAI(model="models/gemini-2.5-pro",google_api_key=userdata.get('GOOGLE_API_KEY'))

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ## for joinning all the relevant documnets from all documents

# def format_docs(docs):

#   return "\n".join(doc.page_content for doc in docs)

def format_docs(docs):
    # Join all relevant docs into one big text block
    context_text = "\n".join(doc.page_content for doc in docs)

    # Add your special instructions and format the prompt nicely
    prompt_context = f"""
You are a helpful medical assistant. Here is the patient's blood test report:

{context_text}

Answer the user's question based  on this report. Explain any medical terms simply as like a normal people language.
If user ask outside the report but related to report then search it and answer it


"""
    return prompt_context

## we need to join them ,what we get from retriever , we dont pass whole data right

rag_chain = ( { 'context':retriever | format_docs ,'question': RunnablePassthrough()}

                | prompt           # we add our  context and question to prompt
                | llm              # then pass prompt to LLM (openAI())
                | StrOutputParser()    # we need to parse our output from LLM response
              )

rag_chain.invoke("What does a high lymphocyte count mean explain in detail ?")